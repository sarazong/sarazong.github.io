---
title: "Unit Test in Data Engineering"
---

My primary responsibility at my current job is transforming clinical trials data.
Although data cleaning is always important, so that we produce accurate results 
and interpretations from exploratory analysis and models. It's especially important 
to for clinical data since a lot of the time, we will end up submitting our trial 
to FDA for product approval. Having a reliable data pipeline would facilitate the 
review of the clinical trial. 

**Unit testing** in software engineering is the practice to test a component of 
a software to make sure each component is working as it is supposed to. The concept 
of unit testing can also be applied to data engineering. 

This is especially useful for data that is changing constantly, for example, more 
subjects are being added to the database as the clinical trial progress. With 
**continuous integration**, which means the pipeline runs and tests whenever changes 
are pushed to the code base. If changes to the data or code introduce bugs, the 
tests would catch them. 

Imagine we have a very simple data pipeline to load in the raw data, transform/clean 
the data, and then save/upload the cleaned data set(s). Each of these step can be 
validated with unit test and here are some examples of how:

- **load data:** test whether we have the correct raw dataset by checking specific 
column(s) exist in the datasetk, or the number of columns match with expectation

- **data transformation:** specific test would depend on the particular transformation 
step. The idea is to create a mock dataset and use it to test the transformation 
functions. If the transformation is replacing `NA` of a variable with `Not reported`, 
test whether the transformed variable contain any `NA` value. If collapsing 5 categories 
of a variable to 3 categories, test whether the resulting variable contains 3 categories.

- **save/upload cleaned data:** connect to the database and check if the 








